{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports para extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import oandapy as opy\n",
    "import psycopg2\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos datos de precios (Oanda) para el instrumento que elijamos y de twitter (Base de datos de Prophets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricediff = True # Precios o differencia de precios\n",
    "candleformat = 'midpoint' # ['midpoint', 'bidask']\n",
    "instrument = 'USD_JPY'\n",
    "granularity = 'M5'\n",
    "d1 = '2008-01-01'\n",
    "min_window = 5\n",
    "step = int(60/min_window)\n",
    "d2 = str(dt.now())[:10]\n",
    "oanda = opy.API(environment='live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se van a descargar 1979 días\n",
      "Descargando:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dc2f7b9dfd4b868b7ff19e6a85d4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1978), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraemos datos cada 2 días (por simplicidad)\n",
    "\n",
    "dates = pd.date_range(start=d1, end=d2, freq='2D')\n",
    "df = pd.DataFrame()\n",
    "\n",
    "print('Se van a descargar {} días'.format(2*len(dates)))\n",
    "print('Descargando:')\n",
    "\n",
    "pbar = tqdm(total=len(dates) - 1)\n",
    "\n",
    "for i in range(0, len(dates) - 1):\n",
    "    # Oanda toma las fechas en este formato\n",
    "    d1 = str(dates[i]).replace(' ', 'T')\n",
    "    d2 = str(dates[i+1]).replace(' ', 'T')\n",
    "    try:\n",
    "        # Concatenamos cada día en el dataframe\n",
    "        data = oanda.get_history(instrument=instrument, \n",
    "                                 candleFormat=candleformat,\n",
    "                                 start=d1, \n",
    "                                 end=d2, \n",
    "                                 granularity=granularity)\n",
    "        df = df.append(pd.DataFrame(data['candles']))\n",
    "        pbar.update(1)\n",
    "    except:\n",
    "        pass\n",
    "pbar.close()\n",
    "date = pd.DatetimeIndex(df['time'], tz='UTC')\n",
    "df['date'] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['complete', 'time']\n",
    "df = df.drop(drops, axis=1)\n",
    "df = df[100:] # Falla en API\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if candleformat == 'bidask':\n",
    "    \n",
    "    if pricediff:\n",
    "\n",
    "        prices = [j for j in df.columns if j not in ['date', 'volume']]\n",
    "        for i in prices:\n",
    "            df['diff ' + i] = df[i] - df[i].shift(1)\n",
    "        df = df.drop(prices, axis=1)\n",
    "\n",
    "        open_bid = ['diff openBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        open_ask = ['diff openAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        close_bid = ['diff closeBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        close_ask = ['diff closeAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        low_bid = ['diff lowBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        low_ask = ['diff lowAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        high_bid = ['diff highBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        high_ask = ['diff highAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        volume = ['volume' + str(min_window*(i+1)) for i in range(step)]\n",
    "\n",
    "        shifts = list(range(1,step+1))\n",
    "\n",
    "        for v, ob, oa, cb, ca, lb, la, hb, ha, s in zip(volume,\n",
    "                                                     open_bid, \n",
    "                                                     open_ask, \n",
    "                                                     close_bid, \n",
    "                                                     close_ask, \n",
    "                                                     low_bid, \n",
    "                                                     low_ask, \n",
    "                                                     high_bid, \n",
    "                                                     high_ask, \n",
    "                                                     shifts):\n",
    "            df[v] = df['volume'].shift(s)\n",
    "            df[ob] = df['diff openBid'].shift(s)\n",
    "            df[oa] = df['diff openAsk'].shift(s)\n",
    "            df[cb] = df['diff closeBid'].shift(s)\n",
    "            df[ca] = df['diff closeAsk'].shift(s)\n",
    "            df[lb] = df['diff lowBid'].shift(s)\n",
    "            df[la] = df['diff lowAsk'].shift(s)\n",
    "            df[hb] = df['diff highBid'].shift(s)\n",
    "            df[ha] = df['diff highAsk'].shift(s)\n",
    "\n",
    "    else:\n",
    "\n",
    "        open_bid = ['openBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        open_ask = ['openAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        close_bid = ['closeBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        close_ask = ['closeAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        low_bid = ['lowBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        low_ask = ['lowAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        high_bid = ['highBid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        high_ask = ['highAsk' + str(min_window*(i+1)) for i in range(step)]\n",
    "        volume = ['volume' + str(min_window*(i+1)) for i in range(step)]\n",
    "\n",
    "        shifts = list(range(1,step+1))\n",
    "\n",
    "        for v, ob, oa, cb, ca, lb, la, hb, ha, s in zip(volume,\n",
    "                                                     open_bid, \n",
    "                                                     open_ask, \n",
    "                                                     close_bid, \n",
    "                                                     close_ask, \n",
    "                                                     low_bid, \n",
    "                                                     low_ask, \n",
    "                                                     high_bid, \n",
    "                                                     high_ask, \n",
    "                                                     shifts):\n",
    "            df[v] = df['volume'].shift(s)\n",
    "            df[ob] = df['openBid'].shift(s)\n",
    "            df[oa] = df['openAsk'].shift(s)\n",
    "            df[cb] = df['closeBid'].shift(s)\n",
    "            df[ca] = df['closeAsk'].shift(s)\n",
    "            df[lb] = df['lowBid'].shift(s)\n",
    "            df[la] = df['lowAsk'].shift(s)\n",
    "            df[hb] = df['highBid'].shift(s)\n",
    "            df[ha] = df['highAsk'].shift(s)\n",
    "else:  \n",
    "    if pricediff:\n",
    "\n",
    "        prices = [j for j in df.columns if j not in ['date', 'volume']]\n",
    "        for i in prices:\n",
    "            df['diff ' + i] = df[i] - df[i].shift(1)\n",
    "        df = df.drop(prices, axis=1)\n",
    "\n",
    "        open_ = ['diff openMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        close = ['diff closeMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        low = ['diff lowMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        high = ['diff highMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        volume = ['volume' + str(min_window*(i+1)) for i in range(step)]\n",
    "\n",
    "        shifts = list(range(1,step+1))\n",
    "\n",
    "        for v, o, c, l, h, s in zip(volume,\n",
    "                                     open_, \n",
    "                                     close, \n",
    "                                     low, \n",
    "                                     high, \n",
    "                                     shifts):\n",
    "            df[v] = df['volume'].shift(s)\n",
    "            df[o] = df['diff openMid'].shift(s)\n",
    "            df[c] = df['diff closeMid'].shift(s)\n",
    "            df[l] = df['diff lowMid'].shift(s)\n",
    "            df[h] = df['diff highMid'].shift(s)\n",
    "\n",
    "    else:\n",
    "\n",
    "        open_ = ['openMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        close = ['closeMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        low = ['lowMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        high = ['highMid' + str(min_window*(i+1)) for i in range(step)]\n",
    "        volume = ['volume' + str(min_window*(i+1)) for i in range(step)]\n",
    "\n",
    "        shifts = list(range(1,step+1))\n",
    "\n",
    "        for v, o, c, l, h, s in zip(volume,\n",
    "                                     open_, \n",
    "                                     close, \n",
    "                                     low, \n",
    "                                     high, \n",
    "                                     shifts):\n",
    "            df[v] = df['volume'].shift(s)\n",
    "            df[o] = df['openMid'].shift(s)\n",
    "            df[c] = df['closeMid'].shift(s)\n",
    "            df[l] = df['lowMid'].shift(s)\n",
    "            df[h] = df['highMid'].shift(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[step+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario-rtd/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/usuario-rtd/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df['date'] = df['date'].astype(str)\n",
    "df['d2'] = df['date'].str[14:]\n",
    "df = df[df['d2'] == '00:00+00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df = df.drop('d2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_drop = volume + ['volume', 'date']\n",
    "df['H'] = df.drop(fake_drop, 1).max(axis=1)\n",
    "df['L'] = df.drop(fake_drop, 1).min(axis=1)\n",
    "df['vol'] = df[volume].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].str[:13]\n",
    "pr = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_conn():\n",
    "    \"\"\"\n",
    "    Método que hace la conexión a la base de datos\n",
    "    Args:\n",
    "        conn_creds(dict): diccionario donde vienen las credenciales de\n",
    "        la conexión a la base de datos\n",
    "                         host(str): host que hospeda a la base de datos\n",
    "                         port(str): puerto donde está disponible la base de datos\n",
    "                         user(str): usuario con el que se hará la conexión\n",
    "                         password(str): contraseña del usuario en la BD\n",
    "                         database(str): nombre de la base de datos\n",
    "    Returns:\n",
    "        conn: objeto que contiene la sesión de una conexión a la BD\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.environ['ID_HOST'],\n",
    "            port=os.environ['ID_PORT'],\n",
    "            user=os.environ['ID_USER'],\n",
    "            password=os.environ['ID_PASSWORD'],\n",
    "            database=os.environ['ID_DB'],\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logging.error(error)\n",
    "\n",
    "    return conn\n",
    "\n",
    "def download(conn, query):\n",
    "    \"\"\"\n",
    "    Descarga datos de la base de datos según la consulta insertada\n",
    "    Args:\n",
    "        conn (connection): objeto que contiene la sesión de una\n",
    "                           conexión a la base de datos\n",
    "        query (str): String donde se define el query a ejecutarse\n",
    "    Returns:\n",
    "        df (DataFrame): Tabla con los datos que elegimos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = db_conn()\n",
    "query = '''SELECT date_trunc('hour',cast(date as timestamp)), avg(polarity) polarity, avg(subjectivity) subjectivity\n",
    "           FROM tweets GROUP BY date_trunc('hour',cast(date as timestamp))'''\n",
    "tw = download(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw['date_trunc'] = tw['date_trunc'] + timedelta(hours=5)\n",
    "tw['date_trunc'] = tw['date_trunc'].astype(str)\n",
    "tw['date_trunc'] = tw['date_trunc'].str[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pr, tw, left_on='date', right_on='date_trunc', how='left')\n",
    "df = df.drop(['date', 'date_trunc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(method='ffill')\n",
    "df = df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports para modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier, TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, response, train_size=0.75, time_series=False, scaling=None):\n",
    "    \"\"\"\n",
    "    Regresa train y test sets\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Datos listos para el modelo\n",
    "        response (str): Variable respuesta\n",
    "        train_size (float): % Train Size\n",
    "        time_series (boolean): Si es serie de tiempo o no\n",
    "        scaling (str): ['standard', 'minmax', 'maxabs', 'robust', 'quantile']\n",
    "    Returns:\n",
    "        X_train (Array): conjunto de datos de entrenamiento (indep)\n",
    "        X_test (Array): conjunto de datos de prueba (indep)\n",
    "        y_train (Array): conjunto de datos de entrenamiento (dep)\n",
    "        y_test (Array): conjunto de datos de prueba (dep)\n",
    "    \"\"\"\n",
    "\n",
    "    data = df.copy()\n",
    "    X = data.drop(response, 1)\n",
    "    y = data[response]\n",
    "\n",
    "    logging.info('X columns')\n",
    "    logging.info(list(X.columns))\n",
    "    logging.info('Response')\n",
    "    logging.info(response)\n",
    "\n",
    "    if time_series:\n",
    "        trainsize = int(train_size*len(X))\n",
    "        X_train = X[:trainsize].values\n",
    "        X_test = X[trainsize:].values\n",
    "        y_train = y[:trainsize].values\n",
    "        y_test = y[trainsize:].values\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X.values,\n",
    "                                                            y.values,\n",
    "                                                            random_state=0,\n",
    "                                                            train_size=train_size)\n",
    "    if scaling == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    if scaling == 'minmax':\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    if scaling == 'maxabs':\n",
    "        scaler = preprocessing.MaxAbsScaler()\n",
    "    if scaling == 'robust':\n",
    "        scaler = preprocessing.RobustScaler()\n",
    "    if scaling == 'quantile':\n",
    "        scaler = preprocessing.QuantileTransformer()\n",
    "\n",
    "    if scaling != None:\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pricediff:\n",
    "    response = 'future diff close'\n",
    "    actual = 'diff closeMid'\n",
    "else:\n",
    "    response = 'future close'\n",
    "    actual = 'closeMid'\n",
    "    \n",
    "df[response] = df[actual].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test(df, response, train_size=0.75, time_series=True, scaling=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar MSE\n",
    "# Agregar loop para todas las responses y todos los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpotreg(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Usando TPOT (Tree-Based Pipeline Optimization Tool), librería de AutoML,\n",
    "    genera el \"mejor\" modelo de regresión automáticamente\n",
    "    Args:\n",
    "        X_train (Array): conjunto de datos de entrenamiento (regresores)\n",
    "        y_train (Array): conjunto de datos de entrenamiento (objetivo)\n",
    "    Returns:\n",
    "        tpotmod (modelo): Modelo de regresión generado con TPOT\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline_optimizer = TPOTRegressor(generations=5,\n",
    "                                       population_size=50,\n",
    "                                       cv=5,\n",
    "                                       random_state=42,\n",
    "                                       verbosity=2,\n",
    "                                       n_jobs=4)\n",
    "    tpotmod = pipeline_optimizer.fit(X_train, y_train)\n",
    "\n",
    "    return tpotmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario-rtd/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Optimization Progress:   6%|▌         | 18/300 [11:38<3:37:39, 46.31s/pipeline]Process ForkPoolWorker-11:\n",
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-ab015d8cf05e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtpotmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpotreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-a3a50991f6ad>\u001b[0m in \u001b[0;36mtpotreg\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                        \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                        n_jobs=4)\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtpotmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtpotmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    648\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    639\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;31m# need raise RuntimeError because no pipeline has been optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A pipeline has not yet been optimized. Please call fit() first.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "tpotmod = tpotreg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(range(len(y_test[:30])),tpotmod.predict(X_test[:50]), color='r', label='Predicted')\n",
    "plt.plot(range(len(y_test[:30])),y_test[:50], color='b', label='Expected')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
